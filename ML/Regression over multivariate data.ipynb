{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Prediction over multivariate data\n",
    "An exploratory experiment conducted by Weiye Chen.\n",
    "\n",
    "Todos:\n",
    "* Explore Data before modeling\n",
    "    * ploting and reviewing\n",
    "* Modeling Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadACLoadSample():\n",
    "    with open('./ACLoad.csv', newline='') as csvfile:\n",
    "        table = csv.DictReader(csvfile, delimiter=',', quotechar='|')\n",
    "        ones = []\n",
    "        date = []\n",
    "        hour = []\n",
    "        temp = []\n",
    "        humi = []\n",
    "        sunr = []\n",
    "        k_1temp = []\n",
    "        k_1sunr = []\n",
    "        load = []\n",
    "        for row in table:\n",
    "#             date.append(row['date'])\n",
    "            ones.append(1.0)\n",
    "#             hour.append(float(row['hour']))\n",
    "            temp.append(float(row['temp']))\n",
    "            humi.append(float(row['humi']))\n",
    "            sunr.append(float(row['sunr']))\n",
    "            k_1temp.append(float(row['k_1temp']))\n",
    "            k_1sunr.append(float(row['k_1sunr']))\n",
    "            load.append(float(row['load']))\n",
    "        matrix_t = np.mat([np.array(ones), temp, humi, sunr, k_1temp, k_1sunr, load])\n",
    "        matrix_r = np.transpose(matrix_t)\n",
    "        return matrix_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this notebook environment assumes a data matrix with a row of 1.0 at the top. It's essential for OLS, however, they should be eliminated in the machine learning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.    27.9   21.2 ...   26.8  242.6 -385.7]\n",
      " [   1.    28.8   21.4 ...   27.9  353.3 -399.2]\n",
      " [   1.    29.5   21.2 ...   28.8  491.4 -403.7]\n",
      " ...\n",
      " [   1.    31.3   22.  ...   32.4  167.8 -379.8]\n",
      " [   1.    30.1   21.6 ...   31.3   20.8 -362.7]\n",
      " [   1.    29.    21.3 ...   30.1    0.  -348. ]]\n"
     ]
    }
   ],
   "source": [
    "matrix = loadACLoadSample()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, columns represents ones, outdoor temperature `temp`, humidity `humi`, exposure to sun radiation `sunr`, outdoor temperature at k-1 hour `k_1temp`, exposure to sun radiation at k-1 hour `k_1sunr` and air condition power load `load`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "Using Ordinary Least Squares (OLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLR:     \n",
    "    def __init__(self, x, y = None):\n",
    "        \n",
    "        self.init()\n",
    "        if (y is None):\n",
    "            self.x = np.array(matrix[:,0:-1])\n",
    "            self.y = np.array(matrix[:,-1])\n",
    "            shape = np.shape(matrix)\n",
    "            self.k = shape[1] - 2\n",
    "            self.n = shape[0]\n",
    "        else:\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self.k = np.shape(x)[1] - 1\n",
    "            self.n = np.shape(x)[0]\n",
    "    \n",
    "    def init(self):\n",
    "        self.r2 = 0.\n",
    "        self.t = []\n",
    "        self.p_t = []\n",
    "        self.f = 0.\n",
    "        self.p_f = 0.\n",
    "        self.ess = 0.\n",
    "        self.rss = 0.\n",
    "        self.tss = 0.\n",
    "        self.r2_adj = 0.\n",
    "\n",
    "        \n",
    "    def OLS(self):\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        x_t = np.transpose(x)\n",
    "        xt_x_inv = np.matmul(x_t, x)\n",
    "        xt_x_inv = np.linalg.inv(xt_x_inv)\n",
    "        beta = np.matmul(xt_x_inv, x_t)\n",
    "        beta = np.matmul(beta, y)\n",
    "        self.beta = beta\n",
    "        y_e = self.PredictionE(x)\n",
    "        deviation = self.y - y_e\n",
    "        deviation = deviation * deviation\n",
    "        self.rss = np.sum(deviation)\n",
    "        mean = np.mean(y)\n",
    "        self.tss = np.sum((y-mean) * (y-mean))\n",
    "        self.ess = self.tss - self.rss\n",
    "        self.f = (self.ess / self.k) / (self.rss / (self.n - self.k - 1))\n",
    "        self.r2 = 1 - self.rss / self.tss\n",
    "        self.r2_adj = 1 - (self.rss / (self.n - self.k - 1)) / (self.tss / (self.n - 1))\n",
    "        self.p_f = 1 - stats.f.cdf(self.f, self.k, (self.n - self.k - 1))\n",
    "        sigma2 = self.rss / (self.n - self.k - 1)\n",
    "        beta_array = np.array(np.transpose(beta)[0])\n",
    "        cii = []\n",
    "        for i in range(0, self.k + 1):\n",
    "            cii.append(xt_x_inv[i][i])\n",
    "        cii_np = np.array(cii)\n",
    "        self.t = beta_array / np.sqrt(cii_np * sigma2)\n",
    "        self.p_t = 1 - stats.t.cdf(np.absolute(self.t), self.n - self.k - 1)\n",
    "        \n",
    "    def PredictionE(self, x):\n",
    "        return np.matmul(x, self.beta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = MLR(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr.OLS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.57997287e+02]\n",
      " [-1.04921325e+01]\n",
      " [-1.82365349e+01]\n",
      " [ 7.52564326e-03]\n",
      " [-6.42760743e-01]\n",
      " [ 1.92977516e-02]]\n"
     ]
    }
   ],
   "source": [
    "beta = mlr.beta\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F检验显著性p value\n",
    "\n",
    "t检验及是否处于95%置信区间之外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1102230246251565e-16\n",
      "[ 17.8584512   -4.51603903 -21.69461018   0.85423894  -0.33122165\n",
      "   2.11854944]\n",
      "[ True  True  True False False  True]\n"
     ]
    }
   ],
   "source": [
    "print(mlr.p_f)\n",
    "print(mlr.t)\n",
    "print(mlr.p_t < 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS(matrix):\n",
    "    x = np.array(matrix[:,0:-1])\n",
    "    y = np.array(matrix[:,-1])\n",
    "    x_t = np.transpose(x)\n",
    "    beta = np.matmul(x_t, x)\n",
    "    beta = np.linalg.inv(beta)\n",
    "    beta = np.matmul(beta, x_t)\n",
    "    beta = np.matmul(beta, y)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.8423908522946908\n",
      "R2_adj = 0.8397012763952827\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 = \" + str(mlr.r2))\n",
    "print(\"R2_adj = \" + str(mlr.r2_adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's suggested in the coefficient of determination that Variables and Target are highly linearly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Divide data into two sets with randomness, one for training and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to the machine learning model, we should normalize the data. It's very important as to improve the performance of the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           2.24364937   2.26701032 258.50252051   2.35136086\n",
      " 248.54142779  62.03627259]\n"
     ]
    }
   ],
   "source": [
    "std = np.array(np.std(matrix, axis = 0))[0]\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1.           29.05652174   19.75451505  313.92240803   28.9548495\n",
      "  326.12474916 -317.07658863]\n"
     ]
    }
   ],
   "source": [
    "mean = np.array(np.mean(matrix, axis = 0))[0]\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.51546456  0.63761728  0.15232962 -0.91642654 -0.33605967 -1.10618205]\n",
      " [-0.11433237  0.7258392   0.68656039 -0.44861234  0.10933892 -1.32379667]\n",
      " [ 0.19765934  0.63761728  0.16702967 -0.06585527  0.66498069 -1.39633488]\n",
      " ...\n",
      " [ 0.99992374  0.99050495 -1.13392476  1.46517302 -0.63701553 -1.0110764 ]\n",
      " [ 0.46508081  0.81406111 -1.21438819  0.99735882 -1.22846622 -0.73543122]\n",
      " [-0.02519188  0.68172824 -1.21438819  0.48701606 -1.31215448 -0.49847307]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = preprocessing.scale(matrix[:, 1:])\n",
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivideData(matrix, ratioTesting):\n",
    "    x = np.array(matrix[:,0:-1])\n",
    "    y = np.array(np.transpose(matrix[:,-1]))\n",
    "    print(np.shape(x), np.shape(y))\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=ratioTesting)\n",
    "    return xtrain, xtest, ytrain, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 5) (299,)\n"
     ]
    }
   ],
   "source": [
    "# Global Variables:\n",
    "xtrain, xtest, ytrain, ytest = DivideData(matrix1, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using Machine Learning\n",
    "In this section, we will use several machine Learning Model to regress the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(array, col):\n",
    "    return array * std[col] + mean[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Supporting Vector Machine Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMR(xtrain, ytrain):\n",
    "    clf = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = SVMR(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7962612458051609"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Forest (Emsembled Decision Trees with randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(xtrain, xtest, ytrain, ytest):\n",
    "    regr = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100)\n",
    "    regr.fit(xtrain, ytrain)\n",
    "    return regr.feature_importances_, regr.score(xtest, ytest), regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = randomForest(xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances\n",
      "temp,humi,sunr,k_1temp,k_1sunr,load\n",
      "[0.07698041 0.80087753 0.01528824 0.08568883 0.02116499]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Importances\")\n",
    "print(\"temp,humi,sunr,k_1temp,k_1sunr,load\")\n",
    "print(rf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Score = 0.8310752868015373\n"
     ]
    }
   ],
   "source": [
    "print(\"Regression Score = \" + str(rf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                      n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiLayerPerceptron(xtrain, xtest, ytrain, ytest):\n",
    "    clf = MLPRegressor(solver='lbfgs', activation = 'logistic', alpha=1e-6, hidden_layer_sizes=(5, 5), random_state=1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = multiLayerPerceptron(xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8713677079098294"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89556847, -0.34585391, -0.34266279,  1.49362466, -1.51394225,\n",
       "       -0.46249741,  1.78453906, -0.07795122,  1.1570157 ,  1.07000498,\n",
       "       -0.30969489,  1.04286428,  0.93296135, -1.77354117,  1.75325865,\n",
       "       -0.01095141,  1.29542966, -0.81956491,  0.63104349,  1.25427681,\n",
       "        1.64855792, -1.35860491,  0.4104094 ,  1.32427037,  1.13639606,\n",
       "       -1.6938346 , -0.84663744, -0.58819423,  0.45633156, -0.35078152,\n",
       "       -0.15292788, -0.34026327,  1.74105853,  0.86131739,  1.09612553,\n",
       "       -0.43551203, -0.49242549,  0.46476272,  0.78963165, -1.64437031,\n",
       "       -0.14658968,  1.29028677, -0.4157555 ,  0.39329235,  1.26002757,\n",
       "        0.32877173,  1.10761753, -1.26606809, -2.26222817, -0.78384978,\n",
       "        0.91400961,  0.97401071, -0.20333799,  1.70932889,  1.27745573,\n",
       "       -0.41985653, -0.42434104,  0.90765711,  0.56818347,  1.63228049,\n",
       "        1.0490695 ,  0.69849221,  0.1337964 , -0.15656692,  0.26586269,\n",
       "        0.14764795, -1.03739094,  0.9440404 ,  0.7729561 , -0.68355233,\n",
       "        0.88882776, -0.69969479, -0.29653719, -0.55164177, -0.41360561])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-372.6343186 , -338.53207623, -338.33411059, -224.41768202,\n",
       "       -410.99592294, -345.76820387, -206.37043695, -321.91239198,\n",
       "       -245.29964756, -250.69746817, -336.28890509, -252.38117566,\n",
       "       -259.19914405, -427.10047238, -208.31095711, -317.75597298,\n",
       "       -236.71296104, -367.919341  , -277.92900275, -239.26593035,\n",
       "       -214.80619983, -401.35937335, -291.61631904, -234.92379085,\n",
       "       -246.57881273, -422.15577359, -369.59881967, -353.56596608,\n",
       "       -288.76747963, -338.83776647, -326.56366402, -338.18525333,\n",
       "       -209.0678073 , -263.64366835, -249.07704661, -344.09413141,\n",
       "       -347.62483075, -288.24444172, -268.09078412, -419.08719371,\n",
       "       -326.17046598, -237.03200696, -342.86851031, -292.67819696,\n",
       "       -238.9091747 , -296.68081615, -248.36412546, -395.61873398,\n",
       "       -457.41679221, -365.70370725, -260.37483903, -256.65259468,\n",
       "       -329.69091975, -211.03619557, -237.82799674, -343.12292259,\n",
       "       -343.40112531, -260.76892483, -281.82860375, -215.81599102,\n",
       "       -251.99622717, -273.7447352 , -308.77635864, -326.78941677,\n",
       "       -300.58345853, -307.91706006, -381.43245557, -258.51184122,\n",
       "       -269.12527307, -359.48162712, -261.93702751, -360.48304529,\n",
       "       -335.47265033, -351.29838789, -342.73513914])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denormalize(mlp.predict(xtest),-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Neural Network (MLP) needs to be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Summary\n",
    "The predictions achieved by three machine learning models don't significantly outperform the multiple linear regression.\n",
    "\n",
    "Special Thanks to Anlin Li for providing a time dependent dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
